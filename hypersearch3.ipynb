{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import Deep3D_branched as deep3d\n",
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly select a subset of data as validation set for this\n",
    "left_dir = \"/a/data/deep3d_data/frames2/left/\"\n",
    "right_dir = \"/a/data/deep3d_data/frames2/right/\"\n",
    "\n",
    "np.random.seed(500)\n",
    "left_files = [left_dir + fname for fname in np.random.choice(os.listdir(left_dir), size = 2500)]\n",
    "np.random.seed(500)\n",
    "right_files = [right_dir + fname for fname in np.random.choice(os.listdir(right_dir), size = 2500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Search Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations needed:  1800\n"
     ]
    }
   ],
   "source": [
    "# Search Scope\n",
    "batchsize = 50\n",
    "num_epochs = 1\n",
    "num_batches = (len(left_files)/batchsize)*num_epochs \n",
    "\n",
    "learning_rates = [0.005,0.0014, 0.0007, 0.0003]\n",
    "beta1 = np.linspace(0.85,0.95,3)\n",
    "beta2 = np.linspace(0.94,0.999,3)\n",
    "\n",
    "search_count = len(learning_rates) * len(beta1) * len(beta2)\n",
    "\n",
    "print \"Iterations needed: \", search_count*num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_name = \"hypercost.p\"\n",
    "pickle.dump({}, open(pickle_name, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Search Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.005 | beta1: 0.85 | beta2: 0.94\n",
      "Finished hyperparam: 1 of 36\n",
      "learning_rate: 0.005 | beta1: 0.85 | beta2: 0.9695\n",
      "Finished hyperparam: 2 of 36\n",
      "learning_rate: 0.005 | beta1: 0.85 | beta2: 0.999\n",
      "Finished hyperparam: 3 of 36\n",
      "learning_rate: 0.005 | beta1: 0.9 | beta2: 0.94\n",
      "Finished hyperparam: 4 of 36\n",
      "learning_rate: 0.005 | beta1: 0.9 | beta2: 0.9695\n",
      "Finished hyperparam: 5 of 36\n",
      "learning_rate: 0.005 | beta1: 0.9 | beta2: 0.999\n",
      "Finished hyperparam: 6 of 36\n",
      "learning_rate: 0.005 | beta1: 0.95 | beta2: 0.94\n",
      "Finished hyperparam: 7 of 36\n",
      "learning_rate: 0.005 | beta1: 0.95 | beta2: 0.9695\n",
      "Finished hyperparam: 8 of 36\n",
      "learning_rate: 0.005 | beta1: 0.95 | beta2: 0.999\n",
      "Finished hyperparam: 9 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.85 | beta2: 0.94\n",
      "Finished hyperparam: 10 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.85 | beta2: 0.9695\n",
      "Finished hyperparam: 11 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.85 | beta2: 0.999\n",
      "Finished hyperparam: 12 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.9 | beta2: 0.94\n",
      "Finished hyperparam: 13 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.9 | beta2: 0.9695\n",
      "Finished hyperparam: 14 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.9 | beta2: 0.999\n",
      "Finished hyperparam: 15 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.95 | beta2: 0.94\n",
      "Finished hyperparam: 16 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.95 | beta2: 0.9695\n",
      "Finished hyperparam: 17 of 36\n",
      "learning_rate: 0.0014 | beta1: 0.95 | beta2: 0.999\n",
      "Finished hyperparam: 18 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.85 | beta2: 0.94\n",
      "Finished hyperparam: 19 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.85 | beta2: 0.9695\n",
      "Finished hyperparam: 20 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.85 | beta2: 0.999\n",
      "Finished hyperparam: 21 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.9 | beta2: 0.94\n",
      "Finished hyperparam: 22 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.9 | beta2: 0.9695\n",
      "Finished hyperparam: 23 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.9 | beta2: 0.999\n",
      "Finished hyperparam: 24 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.95 | beta2: 0.94\n",
      "Finished hyperparam: 25 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.95 | beta2: 0.9695\n",
      "Finished hyperparam: 26 of 36\n",
      "learning_rate: 0.0007 | beta1: 0.95 | beta2: 0.999\n",
      "Finished hyperparam: 27 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.85 | beta2: 0.94\n",
      "Finished hyperparam: 28 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.85 | beta2: 0.9695\n",
      "Finished hyperparam: 29 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.85 | beta2: 0.999\n",
      "Finished hyperparam: 30 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.9 | beta2: 0.94\n",
      "Finished hyperparam: 31 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.9 | beta2: 0.9695\n",
      "Finished hyperparam: 32 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.9 | beta2: 0.999\n",
      "Finished hyperparam: 33 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.95 | beta2: 0.94\n",
      "Finished hyperparam: 34 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.95 | beta2: 0.9695\n",
      "Finished hyperparam: 35 of 36\n",
      "learning_rate: 0.0003 | beta1: 0.95 | beta2: 0.999\n",
      "Finished hyperparam: 36 of 36\n"
     ]
    }
   ],
   "source": [
    "# Define config for GPU memory debugging \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True  # Switch to True for dynamic memory allocation instead of TF hogging BS\n",
    "config.gpu_options.per_process_gpu_memory_fraction= 1  # Cap TF mem usage\n",
    "config.allow_soft_placement=True\n",
    "\n",
    "count = 0\n",
    "for lr in learning_rates:\n",
    "    for b1 in beta1:\n",
    "        for b2 in beta2:\n",
    "            print 'learning_rate: ' + str(lr) + ' | beta1: ' + str(b1) + ' | beta2: ' + str(b2)\n",
    "            # Queue Stuff remains invariant\n",
    "            with tf.device('/cpu:0'):\n",
    "                left_image_queue = tf.train.string_input_producer(\n",
    "                  left_dir + tf.convert_to_tensor(os.listdir(left_dir)),\n",
    "                  shuffle=False)\n",
    "                right_image_queue = tf.train.string_input_producer(\n",
    "                  right_dir + tf.convert_to_tensor(os.listdir(right_dir)),\n",
    "                  shuffle=False)\n",
    "\n",
    "                # use reader to read file\n",
    "                image_reader = tf.WholeFileReader()\n",
    "\n",
    "                _, left_image_raw = image_reader.read(left_image_queue)\n",
    "                left_image = tf.image.decode_jpeg(left_image_raw)\n",
    "                left_image = tf.cast(left_image, tf.float32)/255.0\n",
    "\n",
    "                _, right_image_raw = image_reader.read(right_image_queue)\n",
    "                right_image = tf.image.decode_jpeg(right_image_raw)\n",
    "                right_image = tf.cast(right_image, tf.float32)/255.0\n",
    "\n",
    "                left_image.set_shape([160,288,3])\n",
    "                right_image.set_shape([160,288,3])\n",
    "\n",
    "                # preprocess image\n",
    "                batch = tf.train.shuffle_batch([left_image, right_image], \n",
    "                                               batch_size = batchsize,\n",
    "                                               capacity = 12*batchsize,\n",
    "                                               num_threads = 1,\n",
    "                                               min_after_dequeue = 4*batchsize)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Session\n",
    "            sess = tf.Session(config=config)\n",
    "            \n",
    "            #initialize list to store outputs of run\n",
    "            cost_list = []\n",
    "\n",
    "            # Placeholders\n",
    "            images = tf.placeholder(tf.float32, [None, 160, 288, 3], name='input_batch')\n",
    "            true_out = tf.placeholder(tf.float32, [None, 160, 288, 3] , name='ground_truth')\n",
    "            train_mode = tf.placeholder(tf.bool, name='train_mode')\n",
    "\n",
    "            # Building Net based on VGG weights \n",
    "            net = deep3d.Deep3Dnet('./vgg19.npy', dropout = 0.5)\n",
    "            net.build(images, train_mode)\n",
    "\n",
    "            # Define Training Objectives\n",
    "            with tf.variable_scope(\"Loss\"):\n",
    "                cost = tf.reduce_sum(tf.abs(net.prob - true_out))/batchsize\n",
    "                tf.summary.scalar('cost', cost)\n",
    "                \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):    \n",
    "                train = tf.train.AdamOptimizer(learning_rate=lr,beta1=b1, beta2=b2).minimize(cost) \n",
    "            \n",
    "            # Run initializer \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            coord = tf.train.Coordinator()\n",
    "            queue_threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "            \n",
    "            # Tensorboard operations to compile summary and then write into logs\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter('./tensorboard_logs/'+ \n",
    "                                           'lr' + str(lr) + \n",
    "                                           'b1' + str(b1) +\n",
    "                                           'b2' + str(b2) +\n",
    "                                           '/', graph = sess.graph)\n",
    "            \n",
    "            \n",
    "            # Begin Training\n",
    "            next_batch = sess.run(batch)\n",
    "            for i in xrange(num_batches):\n",
    "                _, cost_val,next_batch, summary = sess.run([train, cost, batch, merged],\n",
    "                                                 feed_dict={images: next_batch[0],\n",
    "                                                            true_out: next_batch[1],\n",
    "                                                            train_mode: True})\n",
    "\n",
    "                writer.add_summary(summary, i)\n",
    "                cost_list.append(cost_val)\n",
    "            \n",
    "            count += 1\n",
    "            print \"Finished hyperparam: \" + str(count) + ' of ' + str(search_count)\n",
    "            \n",
    "            \n",
    "            cost_key = (lr, b1, b2)\n",
    "            cost_file = pickle.load(open(pickle_name, \"rb\"))\n",
    "            cost_file[cost_key] = cost_list\n",
    "            pickle.dump(cost_file, open(pickle_name, \"wb\"))\n",
    "            \n",
    "            \n",
    "            sess.close()\n",
    "            tf.reset_default_graph()\n",
    "            coord.request_stop()\n",
    "            coord.join(queue_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
